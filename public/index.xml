<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Hadley on Daniel Hadley</title>
    <link>/</link>
    <description>Recent content in Daniel Hadley on Daniel Hadley</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0600</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Another Failed Attempt to Find the Op-Ed&#39;s Author with Data Science</title>
      <link>/op-ed-author/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/op-ed-author/</guid>
      <description>&lt;p&gt;Last night as I was dozing off, I had a sudden inclination to try and add to the &lt;a href=&#34;http://varianceexplained.org/r/op-ed-text-analysis/&#34;&gt;attempts&lt;/a&gt; that have been made to identify the anonymous &lt;a href=&#34;https://www.nytimes.com/2018/09/05/opinion/trump-white-house-anonymous-resistance.html&#34;&gt;New York Times op-ed&lt;/a&gt; writer. I’ve had &lt;a href=&#34;https://www.danielphadley.com/gone-girl-prediction/&#34;&gt;some success&lt;/a&gt; in the past with machine learning and stylometry. And this is one of the most intriguing authorship questions in years.&lt;/p&gt;
&lt;p&gt;By 2:00 am I was convinced the data had singled out Mike Pence. I even started to wonder what the ethical thing to do is when one has hard evidence about the source of text whose author wished to remain anonymous.&lt;/p&gt;
&lt;p&gt;By 3:00 I was starting to question whether Jon Huntsman was the author. And at 4:00 I finally went to bed, discouraged by the data. I now believe that the text of the op-ed is too short, and that the models are too sensitive to small differences in vocabulary and style to arrive at any solid conclusions.&lt;/p&gt;
&lt;p&gt;I considered scrapping all of this, but there are enough data scientists who also had this thought (hopefully not at midnight), that my friend Jon convinced me to share my failures. So I will now open &lt;a href=&#34;http://blogs.discovermagazine.com/neuroskeptic/2016/03/17/open-the-file-drawer/&#34;&gt;the file drawer&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;imposters-and-the-stylo-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imposters and the Stylo Package&lt;/h2&gt;
&lt;p&gt;The most recent version of the “stylo” package &lt;a href=&#34;https://computationalstylistics.github.io/blog/imposters/&#34;&gt;has a method&lt;/a&gt; for identifying a text’s likely author relative to a group of “impostors,” i.e., authors who were unlikely to have written it. By comparing the text in question against a candidate corpus, the algorithm seeks to find the most similar set of texts. This method was used, for example, to study Julius Caesar’s disputed writings, and to ascribe authorship of “The Cuckoo’s Calling” to JK Rowling.&lt;/p&gt;
&lt;p&gt;The main difference in this case is that there is not much useful data to train on. We only have 957 words from the op-ed, and it’s likely that even these have been edited. Moreover, we do not have many documents from the candidate group that we &lt;em&gt;know&lt;/em&gt; were written by them. More searching may turn up better training material, but I was only able to find an op-ed and a few speeches from Huntsman, a press briefing from John Kelly, and several old blog articles from Mike Pence.&lt;/p&gt;
&lt;p&gt;With that in mind, I compiled the material and ran it through the General Impostors method (GI). I kept the candidate list small, at first, to see if this was going to be worth losing another hour of sleep. I included Pence and Kelly because they are frequently mentioned, and Huntsman because Slate published a &lt;a href=&#34;https://slate.com/news-and-politics/2018/09/new-york-times-op-ed-anonymous-writer-trump.html&#34;&gt;compelling article&lt;/a&gt; implicating him.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stylo)

# loading the files from a specified directory:
tokenized.texts = load.corpus.and.parse(files = &amp;quot;all&amp;quot;, corpus.dir = &amp;quot;../../data/2018 corpus/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## loading anon_oped    ...
## loading huntsman_innaguration    ...
## loading huntsman_outgoing    ...
## loading huntsman_trib    ...
## loading kelly    ...
## loading pence_clinton-speech ...
## loading pence_schools    ...
## loading pence_titanic    ...
## slicing input text into tokens...
## turning words into features, e.g. char n-grams (if applicable)...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# computing a list of most frequent words (trimmed to top 200 items):
features = make.frequency.list(tokenized.texts, head = 200)

# producing a table of relative frequencies:
data = make.table.of.frequencies(tokenized.texts, features, relative = TRUE)

imposters(reference.set = data[-c(1),], test = data[1,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## huntsman    kelly    pence 
##     0.07     0.00     0.97&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Holy moly!! The GI algorithm scores candidates on a 0-1 scale, and Pence has just received a perfect score. Do I call the media? Bury my laptop and take an oath of silence?! Not so fast, because next I started to subtract words from the test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading the files from a specified directory:
tokenized.texts = load.corpus.and.parse(files = &amp;quot;all&amp;quot;, corpus.dir = &amp;quot;../../data/2018 corpus/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## loading anon_oped    ...
## loading huntsman_innaguration    ...
## loading huntsman_outgoing    ...
## loading huntsman_trib    ...
## loading kelly    ...
## loading pence_clinton-speech ...
## loading pence_schools    ...
## loading pence_titanic    ...
## slicing input text into tokens...
## turning words into features, e.g. char n-grams (if applicable)...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# computing a list of most frequent words (this time only the top 50):
features = make.frequency.list(tokenized.texts, head = 25)

# producing a table of relative frequencies:
data = make.table.of.frequencies(tokenized.texts, features, relative = TRUE)


imposters(reference.set = data[-c(1),], test = data[1,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## huntsman    kelly    pence 
##     0.95     0.03     0.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first analysis relied on a larger vocabulary, including several words that were not in the op-ed. Simply by limiting the data to stop-words and basic terms, I have identified Huntsman as the most likely author. The variance is disconcerting, to say the least.&lt;/p&gt;
&lt;p&gt;This is further evident when one uses n-grams instead of single words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading the files from a specified directory:
tokenized.texts = load.corpus.and.parse(files = &amp;quot;all&amp;quot;, corpus.dir = &amp;quot;../../data/2018 corpus/&amp;quot;, ngram.size = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## loading anon_oped    ...
## loading huntsman_innaguration    ...
## loading huntsman_outgoing    ...
## loading huntsman_trib    ...
## loading kelly    ...
## loading pence_clinton-speech ...
## loading pence_schools    ...
## loading pence_titanic    ...
## slicing input text into tokens...
## turning words into features, e.g. char n-grams (if applicable)...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# computing a list of most frequent words (this time only the top 50):
features = make.frequency.list(tokenized.texts, head = 75)

# producing a table of relative frequencies:
data = make.table.of.frequencies(tokenized.texts, features, relative = TRUE)


imposters(reference.set = data[-c(1),], test = data[1,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## huntsman    kelly    pence 
##     0.43     0.23     0.72&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Huntsman and Pence collaborated to write the op-ed.&lt;/p&gt;
&lt;p&gt;Just kidding. It’s clear to me that, just as David Robinson showed, the small training text means that an analysis of this sort is likely to be sensitive to even small differences in the frequency of words.&lt;/p&gt;
&lt;p&gt;I still think that if someone were to spend more time finding appropriate training data, using more than one method, fine-tuning the parameters, and incorporating other outside evidence, she could probably bring more clarity to the discussion. Ultimately, however, I am not convinced that the now famous 957 words comprise a smoking gun.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Branding and Automating with R Markdown</title>
      <link>/branding-rmarkdown/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/branding-rmarkdown/</guid>
      <description>&lt;p&gt;At the 2018 RStudio conference in San Diego, my colleague Jon and I gave a talk about how we use R Markdown to quickly go from nothing, to analysis, to a branded report that we can pass off to clients. This workflow took some time to set up, but like most automation tasks, has ultimately saved us more time and headache than it cost. If you want to skip to the talk,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rpubs.com/jzadra/rconf2018&#34;&gt;Here&lt;/a&gt; are the slides&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/&#34;&gt;Here&lt;/a&gt; is the repo with a short and long version&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com/resources/videos/branding-and-automating-your-work-with-r-markdown/&#34;&gt;Here&lt;/a&gt; is a video (complete with my cringeworthy Hadley joke)&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://github.com/Sorenson-Impact/sorensonimpact&#34;&gt;here&lt;/a&gt; is the package we describe&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;ten-years-ago&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ten Years Ago&lt;/h2&gt;
&lt;p&gt;These talks describe our current workflow, but I like to compare the present to the past. I have no way to quantify this, but I feel like R is uniqe in how quickly its packages and IDE have evolved the last few years. I’m sure there are other examples of this, but R must be in the upper quartile of ancillary developments, e.g., the Tidyverse, RStudio v1, Shiny, and our topic, R Markdown. In evolutionary terms, we skipped from Xbox to Xbox One X.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media0.giphy.com/media/RluM0kvZXkLS0/source.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;When I began as an analyst in local government, there was no easy way to go from code to output. It was common to create a bunch of graphs with a vanilla R script and then painstakingly add them to either a word document or PowerPoint. There were, of course, those who mastered LaTeX as part a painful dissertation writing process, but most people divided workflows between coding and presentation.&lt;/p&gt;
&lt;p&gt;Then came Knitr. Yihui’s work to integrate R with HTML, LaTeX, Markdown and other formats opened up a plethora of possibilities. For instance, &lt;a href=&#34;http://danielphadley.com/How-To-Dashboard-R/&#34;&gt;I found a way&lt;/a&gt; to create a dashboard by knitting analysis into Rhtml. It was difficult to integrate the two, writing &lt;code&gt;&amp;lt;!--rinline I(data) --&amp;gt;&lt;/code&gt; everywhere that took a vector. But that simple method has sustained a &lt;a href=&#34;http://archive.somervillema.gov//dashboard/daily.html&#34;&gt;city dashboard&lt;/a&gt; for years now. Without Yihui, I don’t think any of this would have been possible.&lt;/p&gt;
&lt;p&gt;Simultaneously, data science was advancing with iPython notebooks. For the first time, the notebook format closely integrated code and output, allowing analysts to see results in-line and easily share their work as HTML files. Modeled after the scientific process, notebooks took off, despite their challenges. When R Markdown Notebooks came along and &lt;a href=&#34;http://danielphadley.com/Jupyter-to-Rmarkdown/&#34;&gt;solved things&lt;/a&gt; like version control, it was a monumental improvement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;branding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Branding&lt;/h2&gt;
&lt;p&gt;Today with RMarkdown in RStudio, it is easy to turn your analysis into a presentation. We use &lt;a href=&#34;https://rmarkdown.rstudio.com/ioslides_presentation_format.html&#34;&gt;ioslides&lt;/a&gt; with a &lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/blob/master/SI_Rmarkdown_Talk_Rconf/SI_Rmarkdown_Talk_Rconf.Rmd&#34;&gt;little javascript&lt;/a&gt; to brand them. But in San Diego, Yihui said that they are close to adding PowerPoint as an output format.&lt;/p&gt;
&lt;p&gt;For reports, we use Word because it is very simple to customize things like headers, fonts, and colors. In the talk we gave, I went over an example of how to use a &lt;a href=&#34;https://stackoverflow.com/questions/24672111/how-to-add-a-page-break-in-word-document-generated-by-rstudio-markdown&#34;&gt;Word template&lt;/a&gt; to include page breaks. This is part of a &lt;a href=&#34;https://rmarkdown.rstudio.com/articles_docx.html&#34;&gt;more general&lt;/a&gt; approach to branding that involves iterating on a template until your document looks just right.&lt;/p&gt;
&lt;div id=&#34;branding-a-website&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Branding a Website&lt;/h3&gt;
&lt;p&gt;Blogdown is the apotheosis of branding with Rmd. Granted, my site looks similar to the thousands of other websites based on &lt;a href=&#34;https://themes.gohugo.io/academic/&#34;&gt;Hugo Academic&lt;/a&gt;. But that’s only because it is insanely easy to get started with Blogdown, in general, and the Academic theme in particular. All it takes is this line of code: &lt;code&gt;blogdown::new_site(theme = &amp;quot;gcushen/hugo-academic&amp;quot;)&lt;/code&gt;. So far, I have not made any changes to CSS - just a replaced a few hero images and icons - and it already feels unique.&lt;/p&gt;
&lt;p&gt;There are others who have done a much better job at putting their own mark on Hugo themes. My favorite is &lt;a href=&#34;https://amber.rbind.io/&#34;&gt;Amber Thomas’ site&lt;/a&gt;. She is a contributing author to the Blogdown book with a keen eye for design.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;automating&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating&lt;/h2&gt;
&lt;p&gt;For presentations at work, my colleagues and I fell into a routine. We would copy and paste Rmd documents, adding customizations to the header every time we wanted a new feature. After a while, though, we found that the document headers were growing out of control. There were more than 100 lines that did things like,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load libraries&lt;/li&gt;
&lt;li&gt;Set knitr/rmarkdown options&lt;/li&gt;
&lt;li&gt;Set directories&lt;/li&gt;
&lt;li&gt;Define branded color names&lt;/li&gt;
&lt;li&gt;Customize ggplot themes and geoms&lt;/li&gt;
&lt;li&gt;Define custom functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This made it difficult to create a single report, let alone automate dozens of them.&lt;/p&gt;
&lt;p&gt;To remedy the problem, my colleague Jon created an R package with all of our branding and custom functions. This allowed us to take advantage of the &lt;code&gt;draft&lt;/code&gt; function and the &lt;a href=&#34;https://rmarkdown.rstudio.com/developer_document_templates.html&#34;&gt;RStudio features&lt;/a&gt; that support document templates.&lt;/p&gt;
&lt;p&gt;This video shows how quickly and easily we can create a new report now:&lt;/p&gt;
&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt;
&lt;source src=&#34;/img/draft_functions_large.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;With all of that in place, it is much easier to use a single template to create a series of reports. This &lt;code&gt;for&lt;/code&gt; loop is what we used when we were requested to build a unique report showing data trends in every county in Utah.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (county in unique(utah$County)) {
  try(render(&amp;#39;./county_report.Rmd&amp;#39;,
             output_file = paste(county, &amp;quot;County Report.docx&amp;quot;),
             output_dir = &amp;quot;./County_Reports/&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, it is easier than ever to brand your analysis with Rmarkdown. Creating a custom package will allow you to cut down on clutter, and make templates easily accessible. From there, it is simple to automate reports that output to Word, Ioslides, HTML, or any of a dozen other formats.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Branding and automating your work with R Markdown</title>
      <link>/talk/rstudio/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 -0700</pubDate>
      
      <guid>/talk/rstudio/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rpubs.com/jzadra/rconf2018&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; are the slides&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is the repo with a short and long version&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?time_continue=21329&amp;amp;v=ogy7rHWlsQ8&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a video (complete with my cringeworthy Hadley joke)&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://github.com/Sorenson-Impact/sorensonimpact&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; is the package we describe&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Branding your work with R Markdown</title>
      <link>/talk/rug/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 -0700</pubDate>
      
      <guid>/talk/rug/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rpubs.com/jzadra/rconf2018&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; are the slides&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is the repo with a short and long version&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://github.com/Sorenson-Impact/sorensonimpact&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; is the package we describe&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayes&#39; Puzzle Powers</title>
      <link>/bayes-538/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/bayes-538/</guid>
      <description>&lt;p&gt;A while ago, the popular data journalism site 538 posted a challenging &lt;a href=&#34;http://fivethirtyeight.com/features/rock-paper-scissors-double-scissors/&#34;&gt;probability puzzle&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the table in front of you are two coins. They look and feel identical, but you know one of them has been doctored. The fair coin comes up heads half the time while the doctored coin comes up heads 60 percent of the time. How many flips — you must flip both coins at once, one with each hand — would you need to give yourself a 95 percent chance of correctly identifying the doctored coin?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question proved so difficult, in fact, that 538’s talented puzzle master Oliver Roeder gave an &lt;a href=&#34;https://fivethirtyeight.com/features/can-you-beat-the-game-show/&#34;&gt;incorrect answer&lt;/a&gt;. Someone smarter than me noticed this, and then we worked together to verify with &lt;a href=&#34;https://www.reddit.com/r/statistics/&#34;&gt;r/statistics&lt;/a&gt; and notify those who may have cared (but didn’t), including the authors of a paper Oliver cited. &lt;a href=&#34;https://www.reddit.com/r/statistics/comments/76ul99/is_this_538_puzzle_correct_how_many_coins_flips/&#34;&gt;My question to Reddit&lt;/a&gt; contains R code that traces exactly where they went wrong.&lt;/p&gt;
&lt;p&gt;This kind of puzzle is a classic in statistics text books because it uses a trivial problem - flipping coins - as an example of more meaningful questions. The solutions are often given by comparing hypothetical distributions.&lt;/p&gt;
&lt;p&gt;But I wanted to think of this less abstractly: what if you really were sitting in front of two coins, knowing that one has a slight bias? What would be the most efficient way to find out and how long would it take?&lt;/p&gt;
&lt;div id=&#34;reverend-bayes-confronts-the-coins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reverend Bayes Confronts the Coins&lt;/h3&gt;
&lt;p&gt;The first thought I had reading the 538 post was, “this is perfect for sequential Bayesian updating.” The idea undergirding most of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian_statistics&#34;&gt;Bayesian statistics&lt;/a&gt; is to update your prior probabilities with new data, e.g., start with a belief and then change it on every coin toss.&lt;/p&gt;
&lt;p&gt;In this case, we can begin with a fairly strong prior. We know that at least one of the two coins is biased, so the odds are 50/50. If we quickly used Bayes’ Theorem after each flip, how long would it take us to choose the biased coin with 95% confidence?&lt;/p&gt;
&lt;p&gt;This is not a trivial simulation. In theory, you could go on flipping the same side of both coins in perpetuity. Given that, and the subtle difference in the coins, I opted for a simulation that lasted 2 million games. Each game consisted of a speedy specter of Bayes, who flipped coins and updated probabilities until he was certain which one was fair.&lt;/p&gt;
&lt;p&gt;As it turns out, this method took Bayes-bot an average of 74 flips. The cool thing, however, is that it is possible to be 96% confident in as few as eight flips, if the coins land a certain way. The median is 60, and the max in this simulation was 704.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/coin_flipping_hist.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Notice that the correct answer to the original question is 134 flips, which are needed given that the distribution has a long right tail. But it’s neat to see how often in practice Bayes could beat the theoretical number of flips needed. In this case, 88% of the time, Bayes knew before 134 flips.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

#### Method two: Bayesian updating simulation ####
# H = coin1 = fair 

update &amp;lt;- function(prior, coin_randomizer) {
  # This function updates our prior probabilities after looking at both coins
  
  # Flip
  # The biased coin is randomly selected. 
  # They both return .4 or .6 because we plug these figures into Bayes&amp;#39; theorem below
  # But actual heads/tails probabilities are either 50/50 or 60/40
  flip_coin1 &amp;lt;- sample(c(.4,.6), 1, prob = c(coin_randomizer[1], 1 - coin_randomizer[1]))
  flip_coin2 &amp;lt;- sample(c(.4,.6), 1, prob = c(coin_randomizer[2], 1 - coin_randomizer[2]))
  
  # Hypothesis given the data
  # Bayes Theorum = prob(A|X) =
  # Numerator
  # P(X|A)*P(A) /
  # Denominator
  # P(X) = P(X|A)*P(A) + P(X|not A) * P(not A)
  
  # Prob coin1 is fair given flip1
  posterior_after_flip1 &amp;lt;- (.50 * prior) / ((.50 * prior) + flip_coin1 * (1 - prior))
  
  # Prob coin1 is fair given flip2 and the priors informed by flip1
  posterior_after_flip2 &amp;lt;- (flip_coin2 * posterior_after_flip1) / 
    ((flip_coin2 * posterior_after_flip1) + .50 * (1 - posterior_after_flip1))
  
  return(posterior_after_flip2)
}


simulate &amp;lt;- function(x) {
  
  # We randomly assign the coins 1x per simulation
  coin_randomizer &amp;lt;- sample(c(.5, .4), 2)
  
  # Keep track for checking later
  is_fair_coin1 &amp;lt;- if_else(coin_randomizer[1] == .5, 1, .1)
  
  # Start here
  n_flips &amp;lt;- 0
  prior &amp;lt;- .5
  
  while(prior &amp;gt;= .05 &amp;amp; prior &amp;lt;= .95){
    prior &amp;lt;- update(prior, coin_randomizer)
    n_flips &amp;lt;- n_flips + 1
  }
  
  # Just for updating progress
  print(x)
  # Return how long it took to be 95% sure
  return(n_flips)
  
}

n_sims &amp;lt;- 2e6

sim_results &amp;lt;- 1:n_sims %&amp;gt;% 
  map(function(x) simulate(x)) %&amp;gt;% 
  unlist()

summary(sim_results)
hist(sim_results)
sum(sim_results &amp;lt; 134)



# Let&amp;#39;s make sure we get the right # of true positives
test_the_sim &amp;lt;- function(x) {
  
  # We randomly assign the coins 1x per simulation
  coin_randomizer &amp;lt;- sample(c(.5, .4), 2)
  
  # Keep track for checking later
  is_fair_coin1 &amp;lt;- if_else(coin_randomizer[1] == .5, 1, .1)
  
  # Start here
  n_flips &amp;lt;- 0
  prior &amp;lt;- .5
  
  while(prior &amp;gt;= .05 &amp;amp; prior &amp;lt;= .95){
    prior &amp;lt;- update(prior, coin_randomizer)
    n_flips &amp;lt;- n_flips + 1
  }
  
  # Just for updating progress
  print(x)
  return(prior * is_fair_coin1)
  
}


# Don&amp;#39;t need as many tests
n_tests &amp;lt;- 20000

test_results &amp;lt;- 1:n_tests %&amp;gt;% 
  map(function(x) test_the_sim(x)) %&amp;gt;% 
  unlist()


true_positivies &amp;lt;- sum(test_results &amp;gt; .95)
true_negatives &amp;lt;- sum(test_results &amp;lt; .004)

(true_negatives + true_positivies) / n_tests

# !It works 96% of the time&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Empirical Bayes to Estimate NBA Treys</title>
      <link>/empirical_bayes_nba_treys/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/empirical_bayes_nba_treys/</guid>
      <description>&lt;p&gt;In this post, I describe how I used Emprirical Bayesian methods to estimate the accuracy of NBA three-point shooters. This analysis closely follows the process outlined by David Robinson in his excellent book &lt;a href=&#34;http://varianceexplained.org/r/empirical-bayes-book/&#34;&gt;Introduction to Empirical Bayes: Examples from Baseball Statistics&lt;/a&gt;, and is performed using his &lt;a href=&#34;https://github.com/dgrtwo/ebbr&#34;&gt;ebbr&lt;/a&gt; package in R.^ The goal is to make a reasoned ranking of the top sharp shooters, despite insonsistent and imperfect records of how often players make the shots they attempt.&lt;/p&gt;
&lt;p&gt;To create the following chart, I utilize data from both regular season and playoff records going back to 1980. As I describe below, however, I systematically drop three seasons of records in order to correct for a time when the three-point line was moved closer to the basket. Empirical Bayes is relatively robust to issues like these because estimates are created using data from the larger group: in this case, the entire history of NBA three-point shooters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/first_graph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;NBA fans will quickly recognize Steph Curry at the top of this chart. He is the undisputed king of threes, but not neccesarily because of his accuracy so much as his quantity. Steph’s placement makes more sense in that context. Empirical Bayes effectively shrinks outliers towards a mean or trend; but the shrinking is less significant in cases where there are more observarions. In this case, Steph’s copious threes make us confident in his accuracy, which can be observed in the small error bar.&lt;/p&gt;
&lt;p&gt;On the other hand, NBA fans with a little more historical knowledge will wonder why the erstwhile player Steve Kerr is not up there with the three-point prodigy he currently coaches. Is Steph Curry really the most accurate three-point shooter of all time? This analysis suggests so. But I encourage you to read on to see if you agree.&lt;/p&gt;
&lt;div id=&#34;more-than-three-ways&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More than three ways&lt;/h2&gt;
&lt;p&gt;There are many ways to measure three-point accuracy. We could do the most straight forward: divide the number of makes by the number of attempts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;threes %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_attempted = sum(threes_attempted), threes_made = sum(threes_made),
            Player = Player[1]) %&amp;gt;%
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;% 
  top_n(5, wt = three_pct) %&amp;gt;%
  select(Player, threes_made, threes_attempted, three_pct) %&amp;gt;% 
  head() %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Player&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;threes_made&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;threes_attempted&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;three_pct&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Alonzo Bradley&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Alvin Sims&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Coty Clarke&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Eddy Curry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Eric Anderson&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Eric White&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This has an obvious problem. Most record books overcome it by coming up with a seemingly arbitrary filter and then redoing the calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;threes %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_attempted = sum(threes_attempted), threes_made = sum(threes_made),
            Player = Player[1]) %&amp;gt;%
  # Only players with more than 200 threes made in their careers
  filter(threes_made &amp;gt; 200) %&amp;gt;% 
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;% 
  top_n(10, wt = three_pct) %&amp;gt;%
  select(Player, threes_made, threes_attempted, three_pct) %&amp;gt;%
  arrange(-three_pct) %&amp;gt;%
  mutate(three_pct = paste0((round(three_pct * 100, 2)),&amp;quot;%&amp;quot;)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Player&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;threes_made&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;threes_attempted&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;three_pct&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Steve Kerr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;816&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1852&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;44.06%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hubert Davis&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;837&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1906&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;43.91%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Jason Kapono&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;472&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1082&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;43.62%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Steve Novak&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;637&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1468&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;43.39%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Stephen Curry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2507&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5807&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;43.17%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Kyle Korver&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2723&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6391&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;42.61%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Steve Nash&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1863&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4377&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;42.56%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Drazen Petrovic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;289&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;682&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;42.38%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B.J. Armstrong&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;503&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1189&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;42.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Tim Legler&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;282&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;669&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;42.15%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This method has its own problems. For one, there will always be decent players on the other side of the artbitrary line. Less obviously, there are players with relatively short careers who made a high number of threes, but not enough to inspire confidence that their record would continue through a longer career.&lt;/p&gt;
&lt;p&gt;Emprical Bayes can overcome these problems by generating a “prior” that is informed by all of the players, and then updating with each player’s actual data. A prior is a defensible estimate we would make before seeing an individual’s three-point data. Updating is a simple yet robust way of combining our first empirical guess with the data.&lt;/p&gt;
&lt;p&gt;In Dr. Robinson’s &lt;em&gt;ebbr&lt;/em&gt; package, this can be done with one function: &lt;code&gt;add_ebb_estimate()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic &amp;lt;- threes %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_made = sum(threes_made), threes_attempted = sum(threes_attempted),
            Player = Player[1]) %&amp;gt;% 
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;% 
  add_ebb_estimate(threes_made, threes_attempted, prior_subset = threes_made &amp;gt; 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting both the measured rate and the EB estimate shows that some players are shrunken more towards the mean than others. (Hat tip to Julia Silge, from whom I cribbed the clean looking ggplot code).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic %&amp;gt;%
  arrange(-.fitted) %&amp;gt;% 
  top_n(20, wt = .fitted) %&amp;gt;% 
  mutate(rank = row_number()) %&amp;gt;% 
  rename(&amp;#39;Measured rate&amp;#39; = three_pct, &amp;#39;Empirical Bayes estimate&amp;#39; = .fitted) %&amp;gt;% 
  gather(type, rate, `Measured rate`, `Empirical Bayes estimate`) %&amp;gt;%
  ggplot(aes(rate, reorder(Player, -rank), color = type)) +
  geom_errorbarh(aes(xmin = .low, xmax = .high), color = &amp;quot;gray50&amp;quot;) +
  geom_point(size = 3) +
  labs(x = &amp;quot;Three Point %&amp;quot;,
       y = NULL, title = &amp;quot;Bayesian Estimate of the NBA&amp;#39;s Most Accurate 3-point Shooters&amp;quot;) +
  scale_x_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Steph’s brother Seth, for instance, has a record of hitting almost 44% of his threes, yet we estimate his rate at closer to 41%. This is fundamental to Bayesian anaysis: extreme results require more data before we bet on them at Vegas. In the meantime, we lean more on the prior and draw a larger “credible interval,” which is the region where we very confident that the true value lies.&lt;/p&gt;
&lt;p&gt;At this point, one might wonder why shrink at all? Why not just trust that Seth, Drazen, and others are as accurate as their records suggest they are? The answer is that they are on the far right tail of a fairly consistent density curve. If they got there on the basis of sparse data, it’s only logical to question whether we are seeing a consistent trend or statistical noise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;threes %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_attempted = sum(threes_attempted), threes_made = sum(threes_made),
            Player = Player[1]) %&amp;gt;%
  # Put filter on it to avoid crazy outliers
  filter(threes_made &amp;gt; 100) %&amp;gt;% 
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;%
  ggplot(aes(x = three_pct)) +
  geom_histogram() +
  labs(x = &amp;quot;Three Point %&amp;quot;,
       y = NULL, title = &amp;quot;Histogram of all Career 3-point Averages&amp;quot;,
       subtitle = &amp;quot;for players with more than 100 3s made&amp;quot;) +
  scale_x_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-trouble-with-threes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Trouble with Threes&lt;/h2&gt;
&lt;p&gt;All sports statistics are imperfect measures of a player’s performance. At best, they show relative differences in how well athletes shoot, steal, and rebound. At worst, they are marred by rule changes and outside factors that either exaggerate or handicap certain subgroups of players.&lt;/p&gt;
&lt;p&gt;The NBA’s measure for three-point accuracy is one of worst kinds of the latter. Between 1995 and 1997, the NBA changed the distance of the three-point line, biasing all future comparisons between generations of players. During those years, it was as though we changed the distance of marathons to 21 miles &lt;em&gt;and kept all of the fastest records&lt;/em&gt;. You can see the record jump significantly in 1995.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;threes %&amp;gt;% 
  group_by(season) %&amp;gt;% 
  ggplot(aes(x = season, y = three_pct, group = season)) +
  geom_boxplot() +
  labs(x = &amp;quot;Season&amp;quot;,
       y = NULL, title = &amp;quot;Box Plot of 3% by Season&amp;quot;) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 6148 rows containing non-finite values (stat_boxplot).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To correct for this, I repeated the steps above, but this time I filtered out the years when the line was changed. I also made some important corrections to the prior probabilities similar to ones that Dr. Robinson describes in &lt;a href=&#34;http://varianceexplained.org/r/hierarchical_bayes_baseball/&#34;&gt;this post&lt;/a&gt;. Specifically, I made each player’s prior dependent on the log number of his attempts and the average year he played.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;no_line &amp;lt;- threes %&amp;gt;% 
  mutate(line_change = case_when(season &amp;gt;= 1995 &amp;amp; season &amp;lt;= 1997 ~ &amp;quot;during&amp;quot;,
                                 season &amp;lt; 1995 ~ &amp;quot;before&amp;quot;,
                                 TRUE ~ &amp;quot;after&amp;quot;)) %&amp;gt;% 
  filter(line_change != &amp;quot;during&amp;quot;) %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_made = sum(threes_made), threes_attempted = sum(threes_attempted),
            Player = Player[1], season = round(mean(season))) %&amp;gt;% 
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;% 
  filter(threes_made &amp;gt;= 7) %&amp;gt;% 
  add_ebb_estimate(threes_made, threes_attempted, method = &amp;quot;gamlss&amp;quot;, 
                   mu_predictors = ~ season + log10(threes_attempted)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is the ranking that I started this post with.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-kerr-conundrum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Kerr Conundrum&lt;/h2&gt;
&lt;p&gt;Overall, I am fairly satisfied with the way this ranking turned out. Steph gets the benefit of the doubt, while Jason Kapono’s record gets treated with a little more skepticism. There are a few quibbles I have - most notably, Steve Kerr.&lt;/p&gt;
&lt;p&gt;Kerr had some of his best years during the line change. It’s difficult to reconcile that with the need to compare fairly across generations. The good thing about this method is that it takes all of the available data, including from Kerr’s cohorts. The challenge is that including data from 1995-1997 would bias both the prior and posterior estimates. Moreover, Kerr was an anomaly, even compared to other players from his generation. Therefore, his estimate was shrunken a little more than I am comfortable with. I think he belongs somewhere above the bayesian estimate, but below the top five.&lt;/p&gt;
&lt;p&gt;^ I am indebted to Dr. Robinson both for his lucid explanations of Bayesian statistics and his &lt;em&gt;ebbr&lt;/em&gt; package in R. This post describes a toy example of using Bayesian methods to estimate proportions from hierarchical data, but I have used the same principles in real-world applications. I have worked through a few books on Bayesian data analysis, but none are as as engaging and clear.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deranged X-Mas - a Gift Exchange Puzzle</title>
      <link>/deranged-x-mas/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/deranged-x-mas/</guid>
      <description>&lt;p&gt;While planning a holiday gift exchange this week, my wife casually challenged me with a sort of tricky probability puzzle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sara and I were talking today and realized that we were off by one on the rotation because last year we went sledding instead of buying gifts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;It should actually be: * Sara gives to Jonny * Jimmy gives to Thurop * Amy gives to Lisa * Jonny gives to Sara * Thurop gives to Jimmy * Lisa gives to Amy&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;So everyone is giving to the same person that is giving to them. How often will this phenomenon happen? Dan, give us a statistical analysis ASAP!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Note: they do a rotation, but we will pretend it’s random. Also, names &lt;em&gt;not&lt;/em&gt; changed for privacy. I actually have a brother-in-law named Thurop, which is pretty cool.)&lt;/p&gt;
&lt;p&gt;I fumbled around with permutations for a while before reading that situations like these are actually &lt;a href=&#34;http://mathworld.wolfram.com/Derangement.html&#34;&gt;derangements&lt;/a&gt; - permutations of sets where no element appears in its original position. In the case of a six sibling gift exchange, there are 265 derangements. So now we have the denominator.&lt;/p&gt;
&lt;p&gt;The question is, of these derangements (I will never get tired of using that word, now that I know it), how many contain combinations where the siblings are mirrored, where each is assigned to the same person? To solve the puzzle, my friend Andrew and I took a three phase approach that we often use:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I try analytically and fail&lt;/li&gt;
&lt;li&gt;Andrew succeeds&lt;/li&gt;
&lt;li&gt;I confirm his answer with brute-force programming&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Andrew wrote, “I think the answer is 15/265. The denominator is a derangement, which are confusing as hell, but it is a type of permutation that is calculated: n! * Sum i = 0 -&amp;gt; n of ((-i)^n)/i!. The numerator I’m less sure about.I just counted the total ways that the matches would be mirrored. For person 1 and 6 to be matched there are 3 possibilities, and then there are 5 different people 1 could be matched with, so 5*3.&amp;quot;&lt;/p&gt;
&lt;p&gt;The combinations with person 1, aka Sara, and person 6, aka Lisa, look like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[(1,6)(2,5)(3,4)]&lt;/li&gt;
&lt;li&gt;[(1,6)(2,4)(3,5)]&lt;/li&gt;
&lt;li&gt;[(1,6)(2,3)(4,5)]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can then repeat that chain with siblings 2, 3, 4, and 5.&lt;/p&gt;
&lt;p&gt;To confirm, I started with all derangements and wrote a nested for-loop that looked for cases where the siblings are mirrored. The answer: 15/265, or approximately 5% of the time the gift exchange will work out the way it did this year. It’s another festivus miracle!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gtools)
library(tidyverse)

# First we get the derangements from the permutations
r &amp;lt;- 6
S &amp;lt;- 1:r
ders &amp;lt;- permutations(r, r, S)  
ders &amp;lt;- ders[apply(ders, 1, function(x) !any(x==S)),]

# This df is like a table 
# where the column name recieves from the row
ders &amp;lt;- as.data.frame(ders)
names(ders) &amp;lt;- c(1,2,3,4,5,6)

# start the counter
sibling_mirror &amp;lt;- 0

# iterate over rows of derangements
# and columns representing the recipient
for (giver in 1:nrow(ders)) {
  
  all_pairs &amp;lt;- list()
  
  for (recipient in names(ders)) {
    
    pair &amp;lt;- sort(c(ders[giver, recipient], recipient))
    
    all_pairs[[recipient]] &amp;lt;- pair
    
  }
  
  unique_pairs &amp;lt;- length(unique(all_pairs))
  
  # because I sort the pairs, 
  # if there are only 3 unique ones, that&amp;#39;s a mirror
  if (unique_pairs == 3) {
    sibling_mirror = sibling_mirror + 1
  }
  
}

sibling_mirror / nrow(ders)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Criminal Justice Reform: the Case of Utah</title>
      <link>/talk/dssg/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 -0600</pubDate>
      
      <guid>/talk/dssg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rayid Ghani on Data Science for Social Good</title>
      <link>/interview-rayid-ghani/</link>
      <pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/interview-rayid-ghani/</guid>
      <description>&lt;p&gt;I had a chance to sit down with Rayid Ghani, the Director of the Center for Data Science and Public Policy at the University of Chicago. I have admired Rayid’s work ever since he became the Chief Scientist for the 2012 Barack Obama campaign. His strategic use of analytics set the precedent for a new era of data science in political campaigns. At the time, others were trying to reverse engineer his tactics, which meant he was always one step ahead of the other campaigns. The fact that he has stayed in the field and used data to produce positive social outcomes is truly admirable.&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/XHDuN_2WP7o&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>How to Add a Logo to ggplot by Magick</title>
      <link>/ggplot-logo/</link>
      <pubDate>Tue, 11 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/ggplot-logo/</guid>
      <description>&lt;p&gt;We recently wanted to brand several of our plots for publication in the local press. I looked around and found a &lt;a href=&#34;https://stackoverflow.com/questions/12463691/inserting-an-image-to-ggplot-outside-the-chart-area&#34;&gt;couple&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/41574732/how-to-add-logo-on-ggplot2-footer&#34;&gt;suggestions&lt;/a&gt; on how to add images to plots, but nothing that seemed modular or customizable. My colleague reccomended the relatively new &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;Magick package&lt;/a&gt;, which provided all of the functionality I needed (plus a lot more). Here is a simple example along with the code to replicate it:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/Cars.png&#34; /&gt;

&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(magick)
library(here) # For making the script run without a wd
library(magrittr) # For piping the logo

# Make a simple plot and save it
ggplot(mpg, aes(displ, hwy, colour = class)) + 
  geom_point() + 
  ggtitle(&amp;quot;Cars&amp;quot;) +
  ggsave(filename = paste0(here(&amp;quot;/&amp;quot;), last_plot()$labels$title, &amp;quot;.png&amp;quot;),
         width = 5, height = 4, dpi = 300)

# Call back the plot
plot &amp;lt;- image_read(paste0(here(&amp;quot;/&amp;quot;), &amp;quot;Cars.png&amp;quot;))
# And bring in a logo
logo_raw &amp;lt;- image_read(&amp;quot;http://hexb.in/hexagons/ggplot2.png&amp;quot;) 

# Scale down the logo and give it a border and annotation
# This is the cool part because you can do a lot to the image/logo before adding it
logo &amp;lt;- logo_raw %&amp;gt;%
  image_scale(&amp;quot;100&amp;quot;) %&amp;gt;% 
  image_background(&amp;quot;grey&amp;quot;, flatten = TRUE) %&amp;gt;%
  image_border(&amp;quot;grey&amp;quot;, &amp;quot;600x10&amp;quot;) %&amp;gt;%
  image_annotate(&amp;quot;Powered By R&amp;quot;, color = &amp;quot;white&amp;quot;, size = 30, 
                 location = &amp;quot;+10+50&amp;quot;, gravity = &amp;quot;northeast&amp;quot;)

# Stack them on top of each other
final_plot &amp;lt;- image_append(image_scale(c(plot, logo), &amp;quot;500&amp;quot;), stack = TRUE)
# And overwrite the plot without a logo
image_write(final_plot, paste0(here(&amp;quot;/&amp;quot;), last_plot()$labels$title, &amp;quot;.png&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that this creates a logo centered on a grey background with an annotation originating in the northwest of the band. I wanted to put our logo on the bottom right, 538 style, so I added some superfulous canvas on the right of the png and then croped it to fit our plots with Magick. The point is, Magick is highly customizable and opens up a lot of options that were previously closed to all but the undaunted photoshoppers. Want Vincent Vega to point at your x-axis? Go for it! Now, if you’ll excuse me, I’m going to go home and have a heart attack.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/Cars_Travolta.gif&#34; /&gt;

&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now call back the plot
background &amp;lt;- image_read(paste0(here(&amp;quot;/&amp;quot;), &amp;quot;Cars.png&amp;quot;))
# And bring in a logo
logo_raw &amp;lt;- image_read(&amp;quot;https://i.imgur.com/e1IneGq.jpg&amp;quot;) 

frames &amp;lt;- lapply(logo_raw, function(frame) {
  image_composite(background, frame, offset = &amp;quot;+70+800&amp;quot;)
})

animation &amp;lt;- image_animate(image_join(frames))


image_write(animation, &amp;quot;~/Cars_Travolta.gif&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>RecidiViz: Analyzing Reincarceration</title>
      <link>/project/recidiviz/</link>
      <pubDate>Wed, 31 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/recidiviz/</guid>
      <description>&lt;p&gt;Recidivism is one of society&amp;rsquo;s most persistent yet misunderstood problems. Everyone from politicians to Supreme Court Justices seem to get it wrong. This model uses data from the Bureau of Justice Statistics to simulate 1,000 parolees released on the same day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Prison Analyst&#39;s Dilemma - Using R to Model Recidivism</title>
      <link>/recidiviz/</link>
      <pubDate>Wed, 31 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/recidiviz/</guid>
      <description>&lt;p&gt;Recidivism, the rate at which those released from incarceration return or commit new crimes, is one of society’s most difficult social problems. The official estimate is that 55% of former prisoners will return within 60 months.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://static1.squarespace.com/static/57e1fc9c20099e1414dc6070/t/592890393e00bedb90f8b8da/1495830599102/?format=1000w&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Recidivism is also, I discovered, one of the most challenging things to model and understand statistically. In this blog post, I describe our efforts to build &lt;a href=&#34;https://daniel-hadley.shinyapps.io/Recidivism_App/&#34;&gt;this simulation&lt;/a&gt;, including how we settled on some fairly basic control structures (for loops) without giving up too much in terms of efficiency and readability.&lt;/p&gt;
&lt;div id=&#34;a-common-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Common Problem&lt;/h3&gt;
&lt;p&gt;Recidivism is similar to many other types of social and ecological trends. There is a basic measurement - return to prison in our case, but it could also be website visits, weather patterns, bankruptcy, etc. But unlike, say, mortality rates, which can be modeled using &lt;a href=&#34;https://en.wikipedia.org/wiki/Survivorship_curve&#34;&gt;survivorship curves&lt;/a&gt;, a return to prison is not the end. The vast majority of people who recidivate will return to society, and then a portion of that group will return to prison again, and so on for many cycles.&lt;/p&gt;
&lt;p&gt;If this phenomenon sounds familiar, you may have thought of some type of model you have used in the past. And we looked at many of these, from &lt;a href=&#34;http://setosa.io/ev/markov-chains/&#34;&gt;Markov chains&lt;/a&gt; to state-based models. What we settled on is similar to an agent-based model, but is as simple as two for loops. In pseudocode, it looks like,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(agent in 1:number_of_agents) {
  for(month in 1:number_of_months) {
    Keep track of how long the person has been free
    Calculate the odds of recidivating conditional on months free
    If the agent recidivates, assign a prison sentence 
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this oversimplifies a lot. The “agents” don’t interact, and the probability of recidivating is conditional only on the time someone is free. But it does allow for multiple states, from freedom to prison and back again. And this - combined with the legibility of for loops - makes it easy to add complexity. In another variant of this model, for example, we also added parole sentences, adverse health outcomes, and many other variables.&lt;/p&gt;
&lt;p&gt;The basic functionality can be visualized in this tweenr plot created by my colleague, Sam Nelson:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Sorenson-Impact/Recidivism_App/master/www/model_output.gif&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;for-hate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;For Hate&lt;/h3&gt;
&lt;p&gt;For loops are often pooh-poohed in the R community. They are thought of as slow and inferior to vectorized alternatives. Initially, I could see why. One of our more complex versions with 10,000 agents had build times of “let’s go out to lunch and see where it is when we get back.” When I wanted to do sensitivity analysis with multiple iterations, it was more like, “let’s sleep on it.” Even pared down, our initial version took 13 seconds.&lt;/p&gt;
&lt;p&gt;That’s why I was grateful for Grolemund and Wickham’s chapter on iteration in “R for Data Science.” First, they eased my [r]eligious guilt about our model structure: “Some people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years).”&lt;/p&gt;
&lt;p&gt;Then they provided some useful tips that allowed me to reduce the build time from 13 seconds to 3, without which we could not have built an interactive app. These were simple, but effective: preallocate the output, declare the type of the vector, and save the results in a list, waiting to combine them into a single vector until after the loop is done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-simple-to-complex&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;From Simple to Complex&lt;/h3&gt;
&lt;p&gt;I like this quote by the abolitionist Elizur Wright, “while nothing is more uncertain than a single life, nothing is more certain than the average duration of a thousand lives.” Recidivism trends are not as predictable, but I think that the same principle applies. That is why I like to build models that begin with very simple and uncertain functions, but that build to more tangible outcomes.&lt;/p&gt;
&lt;p&gt;I know that for loops do not come close to capturing the complexity inherent in social systems, but I like that that they mimic the hierarchy, from agents to averages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/Recidivism_App&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Big Data to Solve Big Problems</title>
      <link>/big-data-big-problems/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/big-data-big-problems/</guid>
      <description>&lt;p&gt;In the marketing world, big data is used to answer ostensibly minute questions every day: are computer mouse movements predictive of purchasing? Does an orange background increase user engagement? In every &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_places_with_%22Silicon%22_names&#34;&gt;place with Silicon in its name&lt;/a&gt;, there are teams of data scientists asking these questions.&lt;/p&gt;
&lt;p&gt;In the social sector, by contrast, answering helpful questions is more difficult. For instance, is our program reducing homelessness? How is health spending distributed across the state? Part of the difficulty is the nature of the problems we are trying to solve. Lowering rates of recidivism is far more complex than driving ad clicks.&lt;/p&gt;
&lt;p&gt;But there is another reason that governments, nonprofits, and philanthropists find it difficult to answer basic questions about social services: missing data. We simply do not have the infrastructure to collect and analyze data that reveals true social outcomes.&lt;/p&gt;
&lt;p&gt;We at the Sorenson Impact Center are excited to help address this problem. Alongside &lt;a href=&#34;http://news.stanford.edu/2016/10/17/center-poverty-inequality-lead-program-measure-success-social-programs/&#34;&gt;Raj Chetty&lt;/a&gt; and some excellent organizations, we received a federal grant to help free up, merge, and analyze data to create better social policies. As part of the &lt;a href=&#34;https://nationalservice.tumblr.com/post/151761667357/how-data-and-innovation-can-help-people&#34;&gt;Pay for Success initiative&lt;/a&gt;, the White House has challenged us to assist service providers who will be paid &lt;em&gt;only if&lt;/em&gt; they produce good outcomes.&lt;/p&gt;
&lt;p&gt;The exciting part, from our perspective, is the possibility of using the same tools that were created to optimize things like ad performance in a totally different context. Silicon Valley’s emphasis on data science has led to a proliferation of free, open-source software, a great deal of which can be repurposed in support of Pay for Success projects. The trick is to know how to work with it.&lt;/p&gt;
&lt;p&gt;For instance, data scientists at Google, Twitter and Airbnb have released software that can help &lt;a href=&#34;https://google.github.io/CausalImpact/CausalImpact.html&#34;&gt;estimate causal impacts&lt;/a&gt;, &lt;a href=&#34;https://github.com/twitter/AnomalyDetection&#34;&gt;detect anomalies in time series data&lt;/a&gt;, and interactively &lt;a href=&#34;https://github.com/airbnb/caravel&#34;&gt;visualize large datasets&lt;/a&gt;, respectively. This is part of a broader movement to share code and create tools that others can use. Ultimately, this movement towards open source could give cash-strapped governments and nonprofits access to the same software resources as billion-dollar tech companies.&lt;/p&gt;
&lt;div id=&#34;the-bazaar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Bazaar&lt;/h3&gt;
&lt;p&gt;Open-source software has been described as a &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar&#34;&gt;metaphorical bazaar&lt;/a&gt;. As opposed to a Cathedral — where top-down decisions drive proprietary software — the Bazaar is a place where people from all over the globe can help fix bugs and generate a final product that is free thereafter. In our proposal to the federal government, we volunteered to serve as “guides to the Bazaar,” helping governments and nonprofits identify and learn how to utilize these rich resources.&lt;/p&gt;
&lt;p&gt;At the risk of sounding like techno-utopians, we think this approach of using big data tools has the potential to help solve some of our biggest social problems. One can imagine employing Artificial Neural Networks to identify the parolees who are most in need of social work, or the students who need advanced tutoring. At the very least, we can use packages in the programming languages Python and R to transfer and merge data so that it is useful to service providers.&lt;/p&gt;
&lt;p&gt;If this work sounds appealing to you, please reach out. We are hiring data scientists.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Dashboard with Weak AI</title>
      <link>/project/dashboards/</link>
      <pubDate>Sat, 15 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/dashboards/</guid>
      <description>&lt;p&gt;R has been the perfect language for the back end of &lt;a href=&#34;http://www.somervillema.gov/dashboard/daily.html&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; government data dashboard I am developing.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It has excellent packages to pipe in data from every significant source&lt;/li&gt;
&lt;li&gt;Tools like dplyr and tidyr make cleaning and munging data trivial&lt;/li&gt;
&lt;li&gt;It is ideal for automating analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the R script that powers my dashboard, I have everything from simple averages and frequency tables, to a complex algorithm that converts timeseries figures to Z-Scores and then selects the top 3 variables to display based on standard scores from the last 7 days. (Today that happens to include an uptick in reports of dead animals on the streets and sidewalks, which is disgusting but useful to know).&lt;/p&gt;

&lt;p&gt;This blogpost describes the technical details, and the dashboard&amp;rsquo;s repository is &lt;a href=&#34;https://github.com/cityofsomerville/SomervilleSystems&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Jupyter Notebooks to R Markdown</title>
      <link>/jupyter-to-rmarkdown/</link>
      <pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/jupyter-to-rmarkdown/</guid>
      <description>&lt;p&gt;Edit, 3/28/18: RStudio just announced Python interoperability through the &lt;a href=&#34;https://blog.rstudio.com/2018/03/26/reticulate-r-interface-to-python/&#34;&gt;reticulate package&lt;/a&gt;. Rmd Notebooks are unbeatable, in my opinion.&lt;/p&gt;
&lt;div id=&#34;original-post&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Original Post:&lt;/h3&gt;
&lt;p&gt;I started using Jupyter Notebooks back when they were called IPython. I even remember having to set up a virtual Linux environment because they were not available on Windows. As much as I have enjoyed their functionality, I recently switched entirely to R Markdown in an RStudio environment. Here’s why.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-to-github&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exporting to Github&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;http://rmarkdown.rstudio.com/github_document_format.html&#34;&gt;github_document&lt;/a&gt; option in R Markdown is the perfect way to share your analysis over the web. Just write the markdown with R chunks, “knit” it, and commit everything to a repository, and RStudio will create a directory with all plots and outputs ready to view. One advantage Jupyter Notebooks used to hold was the aesthetic formatting of tables and dataframes on Github. But with the kable function in the knitr package, I have been able to output HTML tables that look just as clean.&lt;/p&gt;
&lt;p&gt;I updated a previous Gist to show how this works &lt;a href=&#34;https://github.com/DanielHadley/Example-R-Markdown-on-Github/blob/master/Example.md&#34;&gt;on Github&lt;/a&gt;. Compare that finished product with the raw code &lt;a href=&#34;https://raw.githubusercontent.com/DanielHadley/Example-R-Markdown-on-Github/master/Example.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-to-word-is-a-breeze&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exporting to Word is a breeze&lt;/h3&gt;
&lt;p&gt;A lot of my analysis is for non-technical clients, many of whom are most comfortable with a Word doc. I have found the process of translating code to .docx to be exquisitely easy. The hardest part was inserting page breaks, but thankfully someone come up with a &lt;a href=&#34;https://scriptsandstatistics.wordpress.com/2015/12/18/rmarkdown-how-to-inserts-page-breaks-in-a-ms-word-document/&#34;&gt;clever hack&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;version-control&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Version control&lt;/h3&gt;
&lt;p&gt;As &lt;a href=&#34;http://opiateforthemass.es/articles/why-i-dont-like-jupyter-fka-ipython-notebook/&#34;&gt;others&lt;/a&gt; have pointed out, version control of a Jupyther Notebook is not a straightforward process. R Markdown is all plain text, so changes are easy to track in Git. For the first time, I feel in control of changes made to my Word documents.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;RStudio&lt;/h3&gt;
&lt;p&gt;I had the R kernel working nicely in Jupyter Notebooks. I did not encounter a lot of errors. But the entire time I was developing, I missed all of the features of RStudio. In particular, I missed having tab completion and the correct indentation on dplyr chains. But I also missed the layout with my environment, history, console, etc., not to mention the View() function on dataframes, and running code line-by-line. RStudio has become an amazing, feature-rich IDE, which I am not anxious to trade for another environment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-is-here-sort-of&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python is here (sort of)&lt;/h3&gt;
&lt;p&gt;At least since &lt;a href=&#34;https://ironholds.org/blog/python-now-in-rstudio/&#34;&gt;2004&lt;/a&gt;, it is possible to run Python scripts in RStudio. There is even syntax highlighting and other IDE features. The nice thing is, the knitr library has support for &lt;a href=&#34;http://yihui.name/knitr/demo/engines/&#34;&gt;other languages&lt;/a&gt;, so you can include Python chunks in, for example, your Word doc.&lt;/p&gt;
&lt;p&gt;There are problems with this, as &lt;a href=&#34;https://www.reddit.com/r/rstats/comments/51tzcb/why_i_switched_from_jupyter_notebooks_to_r/d7k3zvd&#34;&gt;u/_Wintermute noted&lt;/a&gt;. Apparently, the variables do not transfer between “chunks,” let alone languages. The docs for the Python &lt;a href=&#34;http://rmarkdown.rstudio.com/authoring_knitr_engines.html&#34;&gt;language engine&lt;/a&gt; describe a data exchange, which is exciting, but it looks like you have to write the dataframe in one language and then read it in the other. These are not exactly seamless transitions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unknown-unknowns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unknown Unknowns&lt;/h3&gt;
&lt;p&gt;I’m certain there are a lot of great features of both platforms that I am totally unaware of. And I still think Jupyter is doing more to integrate the various data science languages. It’s possible I will switch back, but for now I seem to be getting more locked in to RStudio. At the moment I am researching how to convert my website into R Markdown. It just works really well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
