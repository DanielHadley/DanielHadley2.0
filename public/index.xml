<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Hadley on Daniel Hadley</title>
    <link>/</link>
    <description>Recent content in Daniel Hadley on Daniel Hadley</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Branding and Automating with R Markdown</title>
      <link>/branding-rmarkdown/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/branding-rmarkdown/</guid>
      <description>&lt;p&gt;At the 2018 RStudio conference in San Diego, my colleague Jon and I gave a talk about how we use R Markdown to quickly go from nothing, to analysis, to a branded report that we can pass off to clients. This workflow took some time to set up, but like most automation tasks, has ultimately saved us more time and headache than it cost. If you want to skip to the talk,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rpubs.com/jzadra/rconf2018&#34;&gt;Here&lt;/a&gt; are the slides&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/&#34;&gt;Here&lt;/a&gt; is the repo with a short and long version&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com/resources/videos/branding-and-automating-your-work-with-r-markdown/&#34;&gt;Here&lt;/a&gt; is a video (complete with my cringeworthy Hadley joke)&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://github.com/Sorenson-Impact/sorensonimpact&#34;&gt;here&lt;/a&gt; is the package we describe&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;ten-years-ago&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ten Years Ago&lt;/h2&gt;
&lt;p&gt;These talks describe our current workflow, but I like to compare the present to the past. I have no way to quantify this, but I feel like R is uniqe in how quickly its packages and IDE have evolved the last few years. I’m sure there are other examples of this, but R must be in the upper quartile of ancillary developments, e.g., the Tidyverse, RStudio v1, Shiny, and our topic, R Markdown. In evolutionary terms, we skipped from Xbox to Xbox One X.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media0.giphy.com/media/RluM0kvZXkLS0/source.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;When I began as an analyst in local government, there was no easy way to go from code to output. It was common to create a bunch of graphs with a vanilla R script and then painstakingly add them to either a word document or PowerPoint. There were, of course, those who mastered LaTeX as part a painful dissertation writing process, but most people divided workflows between coding and presentation.&lt;/p&gt;
&lt;p&gt;Then came Knitr. Yihui’s work to integrate R with HTML, LaTeX, Markdown and other formats opened up a plethora of possibilities. For instance, &lt;a href=&#34;http://danielphadley.com/How-To-Dashboard-R/&#34;&gt;I found a way&lt;/a&gt; to create a dashboard by knitting analysis into Rhtml. It was difficult to integrate the two, writing &lt;code&gt;&amp;lt;!--rinline I(data) --&amp;gt;&lt;/code&gt; everywhere that took a vector. But that simple method has sustained a &lt;a href=&#34;http://archive.somervillema.gov//dashboard/daily.html&#34;&gt;city dashboard&lt;/a&gt; for years now. Without Yihui, I don’t think any of this would have been possible.&lt;/p&gt;
&lt;p&gt;Simultaneously, data science was advancing with iPython notebooks. For the first time, the notebook format closely integrated code and output, allowing analysts to see results in-line and easily share their work as HTML files. Modeled after the scientific process, notebooks took off, despite their challenges. When R Markdown Notebooks came along and &lt;a href=&#34;http://danielphadley.com/Jupyter-to-Rmarkdown/&#34;&gt;solved things&lt;/a&gt; like version control, it was a monumental improvement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;branding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Branding&lt;/h2&gt;
&lt;p&gt;Today with RMarkdown in RStudio, it is easy to turn your analysis into a presentation. We use &lt;a href=&#34;https://rmarkdown.rstudio.com/ioslides_presentation_format.html&#34;&gt;ioslides&lt;/a&gt; with a &lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/blob/master/SI_Rmarkdown_Talk_Rconf/SI_Rmarkdown_Talk_Rconf.Rmd&#34;&gt;little javascript&lt;/a&gt; to brand them. But in San Diego, Yihui said that they are close to adding PowerPoint as an output format.&lt;/p&gt;
&lt;p&gt;For reports, we use Word because it is very simple to customize things like headers, fonts, and colors. In the talk we gave, I went over an example of how to use a &lt;a href=&#34;https://stackoverflow.com/questions/24672111/how-to-add-a-page-break-in-word-document-generated-by-rstudio-markdown&#34;&gt;Word template&lt;/a&gt; to include page breaks. This is part of a &lt;a href=&#34;https://rmarkdown.rstudio.com/articles_docx.html&#34;&gt;more general&lt;/a&gt; approach to branding that involves iterating on a template until your document looks just right.&lt;/p&gt;
&lt;div id=&#34;branding-a-website&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Branding a Website&lt;/h3&gt;
&lt;p&gt;Blogdown is the apotheosis of branding with Rmd. Granted, my site looks similar to the thousands of other websites based on &lt;a href=&#34;https://themes.gohugo.io/academic/&#34;&gt;Hugo Academic&lt;/a&gt;. But that’s only because it is insanely easy to get started with Blogdown, in general, and the Academic theme in particular. All it takes is this line of code: &lt;code&gt;blogdown::new_site(theme = &amp;quot;gcushen/hugo-academic&amp;quot;)&lt;/code&gt;. So far, I have not made any changes to CSS - just a replaced a few hero images and icons - and it already feels unique.&lt;/p&gt;
&lt;p&gt;There are others who have done a much better job at putting their own mark on Hugo themes. My favorite is &lt;a href=&#34;https://amber.rbind.io/&#34;&gt;Amber Thomas’ site&lt;/a&gt;. She is a contributing author to the Blogdown book with a keen eye for design.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;automating&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating&lt;/h2&gt;
&lt;p&gt;For presentations at work, my colleagues and I fell into a routine. We would copy and paste Rmd documents, adding customizations to the header every time we wanted a new feature. After a while, though, we found that the document headers were growing out of control. There were more than 100 lines that did things like,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load libraries&lt;/li&gt;
&lt;li&gt;Set knitr/rmarkdown options&lt;/li&gt;
&lt;li&gt;Set directories&lt;/li&gt;
&lt;li&gt;Define branded color names&lt;/li&gt;
&lt;li&gt;Customize ggplot themes and geoms&lt;/li&gt;
&lt;li&gt;Define custom functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This made it difficult to create a single report, let alone automate dozens of them.&lt;/p&gt;
&lt;p&gt;To remedy the problem, my colleague Jon created an R package with all of our branding and custom functions. This allowed us to take advantage of the &lt;code&gt;draft&lt;/code&gt; function and the &lt;a href=&#34;https://rmarkdown.rstudio.com/developer_document_templates.html&#34;&gt;RStudio features&lt;/a&gt; that support document templates.&lt;/p&gt;
&lt;p&gt;This video shows how quickly and easily we can create a new report now:&lt;/p&gt;
&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt;
&lt;source src=&#34;/img/draft_functions_large.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;With all of that in place, it is much easier to use a single template to create a series of reports. This &lt;code&gt;for&lt;/code&gt; loop is what we used when we were requested to build a unique report showing data trends in every county in Utah.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (county in unique(utah$County)) {
  try(render(&amp;#39;./county_report.Rmd&amp;#39;,
             output_file = paste(county, &amp;quot;County Report.docx&amp;quot;),
             output_dir = &amp;quot;./County_Reports/&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, it is easier than ever to brand your analysis with Rmarkdown. Creating a custom package will allow you to cut down on clutter, and make templates easily accessible. From there, it is simple to automate reports that output to Word, Ioslides, HTML, or any of a dozen other formats.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Branding and automating your work with R Markdown</title>
      <link>/talk/rstudio/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 -0800</pubDate>
      
      <guid>/talk/rstudio/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rpubs.com/jzadra/rconf2018&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; are the slides&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is the repo with a short and long version&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?time_continue=21329&amp;amp;v=ogy7rHWlsQ8&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a video (complete with my cringeworthy Hadley joke)&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://github.com/Sorenson-Impact/sorensonimpact&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; is the package we describe&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Branding your work with R Markdown</title>
      <link>/talk/rug/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 -0800</pubDate>
      
      <guid>/talk/rug/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rpubs.com/jzadra/rconf2018&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; are the slides&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/rmarkdown-branding-talk/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is the repo with a short and long version&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://github.com/Sorenson-Impact/sorensonimpact&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; is the package we describe&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayes&#39; Puzzle Powers</title>
      <link>/bayes-538/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/bayes-538/</guid>
      <description>&lt;p&gt;A while ago, the popular data journalism site 538 posted a challenging &lt;a href=&#34;http://fivethirtyeight.com/features/rock-paper-scissors-double-scissors/&#34;&gt;probability puzzle&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the table in front of you are two coins. They look and feel identical, but you know one of them has been doctored. The fair coin comes up heads half the time while the doctored coin comes up heads 60 percent of the time. How many flips — you must flip both coins at once, one with each hand — would you need to give yourself a 95 percent chance of correctly identifying the doctored coin?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The question proved so difficult, in fact, that 538’s talented puzzle master Oliver Roeder gave an &lt;a href=&#34;https://fivethirtyeight.com/features/can-you-beat-the-game-show/&#34;&gt;incorrect answer&lt;/a&gt;. Someone smarter than me noticed this, and then we worked together to verify with &lt;a href=&#34;https://www.reddit.com/r/statistics/&#34;&gt;r/statistics&lt;/a&gt; and notify those who may have cared (but didn’t), including the authors of a paper Oliver cited. &lt;a href=&#34;https://www.reddit.com/r/statistics/comments/76ul99/is_this_538_puzzle_correct_how_many_coins_flips/&#34;&gt;My question to Reddit&lt;/a&gt; contains R code that traces exactly where they went wrong.&lt;/p&gt;
&lt;p&gt;This kind of puzzle is a classic in statistics text books because it uses a trivial problem - flipping coins - as an example of more meaningful questions. The solutions are often given by comparing hypothetical distributions.&lt;/p&gt;
&lt;p&gt;But I wanted to think of this less abstractly: what if you really were sitting in front of two coins, knowing that one has a slight bias? What would be the most efficient way to find out and how long would it take?&lt;/p&gt;
&lt;div id=&#34;reverend-bayes-confronts-the-coins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reverend Bayes Confronts the Coins&lt;/h3&gt;
&lt;p&gt;The first thought I had reading the 538 post was, “this is perfect for sequential Bayesian updating.” The idea undergirding most of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian_statistics&#34;&gt;Bayesian statistics&lt;/a&gt; is to update your prior probabilities with new data, e.g., start with a belief and then change it on every coin toss.&lt;/p&gt;
&lt;p&gt;In this case, we can begin with a fairly strong prior. We know that at least one of the two coins is biased, so the odds are 50/50. If we quickly used Bayes’ Theorem after each flip, how long would it take us to choose the biased coin with 95% confidence?&lt;/p&gt;
&lt;p&gt;This is not a trivial simulation. In theory, you could go on flipping the same side of both coins in perpetuity. Given that, and the subtle difference in the coins, I opted for a simulation that lasted 2 million games. Each game consisted of a speedy specter of Bayes, who flipped coins and updated probabilities until he was certain which one was fair.&lt;/p&gt;
&lt;p&gt;As it turns out, this method took Bayes-bot an average of 74 flips. The cool thing, however, is that it is possible to be 96% confident in as few as eight flips, if the coins land a certain way. The median is 60, and the max in this simulation was 704.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/coin_flipping_hist.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Notice that the correct answer to the original question is 134 flips, which are needed given that the distribution has a long right tail. But it’s neat to see how often in practice Bayes could beat the theoretical number of flips needed. In this case, 88% of the time, Bayes knew before 134 flips.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

#### Method two: Bayesian updating simulation ####
# H = coin1 = fair 

update &amp;lt;- function(prior, coin_randomizer) {
  # This function updates our prior probabilities after looking at both coins
  
  # Flip
  # The biased coin is randomly selected. 
  # They both return .4 or .6 because we plug these figures into Bayes&amp;#39; theorem below
  # But actual heads/tails probabilities are either 50/50 or 60/40
  flip_coin1 &amp;lt;- sample(c(.4,.6), 1, prob = c(coin_randomizer[1], 1 - coin_randomizer[1]))
  flip_coin2 &amp;lt;- sample(c(.4,.6), 1, prob = c(coin_randomizer[2], 1 - coin_randomizer[2]))
  
  # Hypothesis given the data
  # Bayes Theorum = prob(A|X) =
  # Numerator
  # P(X|A)*P(A) /
  # Denominator
  # P(X) = P(X|A)*P(A) + P(X|not A) * P(not A)
  
  # Prob coin1 is fair given flip1
  posterior_after_flip1 &amp;lt;- (.50 * prior) / ((.50 * prior) + flip_coin1 * (1 - prior))
  
  # Prob coin1 is fair given flip2 and the priors informed by flip1
  posterior_after_flip2 &amp;lt;- (flip_coin2 * posterior_after_flip1) / 
    ((flip_coin2 * posterior_after_flip1) + .50 * (1 - posterior_after_flip1))
  
  return(posterior_after_flip2)
}


simulate &amp;lt;- function(x) {
  
  # We randomly assign the coins 1x per simulation
  coin_randomizer &amp;lt;- sample(c(.5, .4), 2)
  
  # Keep track for checking later
  is_fair_coin1 &amp;lt;- if_else(coin_randomizer[1] == .5, 1, .1)
  
  # Start here
  n_flips &amp;lt;- 0
  prior &amp;lt;- .5
  
  while(prior &amp;gt;= .05 &amp;amp; prior &amp;lt;= .95){
    prior &amp;lt;- update(prior, coin_randomizer)
    n_flips &amp;lt;- n_flips + 1
  }
  
  # Just for updating progress
  print(x)
  # Return how long it took to be 95% sure
  return(n_flips)
  
}

n_sims &amp;lt;- 2e6

sim_results &amp;lt;- 1:n_sims %&amp;gt;% 
  map(function(x) simulate(x)) %&amp;gt;% 
  unlist()

summary(sim_results)
hist(sim_results)
sum(sim_results &amp;lt; 134)



# Let&amp;#39;s make sure we get the right # of true positives
test_the_sim &amp;lt;- function(x) {
  
  # We randomly assign the coins 1x per simulation
  coin_randomizer &amp;lt;- sample(c(.5, .4), 2)
  
  # Keep track for checking later
  is_fair_coin1 &amp;lt;- if_else(coin_randomizer[1] == .5, 1, .1)
  
  # Start here
  n_flips &amp;lt;- 0
  prior &amp;lt;- .5
  
  while(prior &amp;gt;= .05 &amp;amp; prior &amp;lt;= .95){
    prior &amp;lt;- update(prior, coin_randomizer)
    n_flips &amp;lt;- n_flips + 1
  }
  
  # Just for updating progress
  print(x)
  return(prior * is_fair_coin1)
  
}


# Don&amp;#39;t need as many tests
n_tests &amp;lt;- 20000

test_results &amp;lt;- 1:n_tests %&amp;gt;% 
  map(function(x) test_the_sim(x)) %&amp;gt;% 
  unlist()


true_positivies &amp;lt;- sum(test_results &amp;gt; .95)
true_negatives &amp;lt;- sum(test_results &amp;lt; .004)

(true_negatives + true_positivies) / n_tests

# !It works 96% of the time&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Empirical Bayes to Estimate NBA Treys</title>
      <link>/empirical_bayes_nba_treys/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/empirical_bayes_nba_treys/</guid>
      <description>&lt;p&gt;All sports statistics are imperfect measures of a player’s performance. At best, they show relative differences in how well athletes shoot, steal, and rebound. At worst, they are marred by rule changes and outside factors that either exaggerate or handicap certain subgroups of players.&lt;/p&gt;
&lt;p&gt;The NBA’s measure for three-point accuracy is one of worst kinds of the latter. Between 1995 and 1997, the NBA changed the distance of the three-point line, biasing all future comparisons between generations of players. During those years, it was as though we changed the distance of marathons to 21 miles &lt;em&gt;and kept all of the fastest records&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What follows is an attemt to use a method called empirical Bayes estimation to rank the most accurate three-point shooters of all time. What makes this different, however, is that I base my estimation on all of the available data &lt;em&gt;except for the seasons where the line was changed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/first_graph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic &amp;lt;- threes %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_made = sum(threes_made), threes_attempted = sum(threes_attempted),
            Player = Player[1]) %&amp;gt;% 
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;% 
  add_ebb_estimate(threes_made, threes_attempted, prior_subset = threes_made &amp;gt; 50)

basic %&amp;gt;% 
  ggplot(aes(.raw, .fitted, color = threes_attempted)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 387 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic %&amp;gt;%
  arrange(-.fitted) %&amp;gt;% 
  top_n(20, wt = .fitted) %&amp;gt;% 
  mutate(rank = row_number()) %&amp;gt;% 
  rename(&amp;#39;Measured rate&amp;#39; = three_pct, &amp;#39;Empirical Bayes estimate&amp;#39; = .fitted) %&amp;gt;% 
  gather(type, rate, `Measured rate`, `Empirical Bayes estimate`) %&amp;gt;%
  ggplot(aes(rate, reorder(Player, -rank), color = type)) +
  geom_errorbarh(aes(xmin = .low, xmax = .high), color = &amp;quot;gray50&amp;quot;) +
  geom_point(size = 3) +
  labs(x = &amp;quot;Three Point %&amp;quot;,
       y = NULL, title = &amp;quot;Measured Rates, Empirical Bayesian Estimates, and Credible Intervals&amp;quot;) +  theme_minimal() +
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;no_line &amp;lt;- threes %&amp;gt;% 
  mutate(line_change = case_when(season &amp;gt;= 1995 &amp;amp; season &amp;lt;= 1997 ~ &amp;quot;during&amp;quot;,
                                 season &amp;lt; 1995 ~ &amp;quot;before&amp;quot;,
                                 TRUE ~ &amp;quot;after&amp;quot;)) %&amp;gt;% 
  filter(line_change != &amp;quot;during&amp;quot;) %&amp;gt;% 
  group_by(player_id) %&amp;gt;% 
  summarise(threes_made = sum(threes_made), threes_attempted = sum(threes_attempted),
            Player = Player[1], season = round(mean(season))) %&amp;gt;% 
  mutate(three_pct = threes_made / threes_attempted) %&amp;gt;% 
  filter(threes_made &amp;gt;= 7) %&amp;gt;% 
  add_ebb_estimate(threes_made, threes_attempted, method = &amp;quot;gamlss&amp;quot;, 
                   mu_predictors = ~ season + log10(threes_attempted)) 

no_line %&amp;gt;% 
  ggplot(aes(.raw, .fitted, color = threes_attempted)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;no_line %&amp;gt;%
  arrange(-.fitted) %&amp;gt;% 
  top_n(20, wt = .fitted) %&amp;gt;% 
  mutate(rank = row_number()) %&amp;gt;% 
  rename(&amp;#39;Measured rate&amp;#39; = three_pct, &amp;#39;Empirical Bayes estimate&amp;#39; = .fitted) %&amp;gt;% 
  gather(type, rate, `Measured rate`, `Empirical Bayes estimate`) %&amp;gt;%
  ggplot(aes(rate, reorder(Player, -rank), color = type)) +
  geom_errorbarh(aes(xmin = .low, xmax = .high), color = &amp;quot;gray50&amp;quot;) +
  geom_point(size = 3) +
  labs(x = &amp;quot;Three Point %&amp;quot;,
       y = NULL, title = &amp;quot;Measured Rates, Empirical Bayesian Estimates, and Credible Intervals&amp;quot;) +  theme_minimal() +
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/empirical_bayes_nba_treys_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deranged X-Mas - a Gift Exchange Puzzle</title>
      <link>/deranged-x-mas/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/deranged-x-mas/</guid>
      <description>&lt;p&gt;While planning a holiday gift exchange this week, my wife casually challenged me with a sort of tricky probability puzzle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sara and I were talking today and realized that we were off by one on the rotation because last year we went sledding instead of buying gifts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;It should actually be: * Sara gives to Jonny * Jimmy gives to Thurop * Amy gives to Lisa * Jonny gives to Sara * Thurop gives to Jimmy * Lisa gives to Amy&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;So everyone is giving to the same person that is giving to them. How often will this phenomenon happen? Dan, give us a statistical analysis ASAP!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Note: they do a rotation, but we will pretend it’s random. Also, names &lt;em&gt;not&lt;/em&gt; changed for privacy. I actually have a brother-in-law named Thurop, which is pretty cool.)&lt;/p&gt;
&lt;p&gt;I fumbled around with permutations for a while before reading that situations like these are actually &lt;a href=&#34;http://mathworld.wolfram.com/Derangement.html&#34;&gt;derangements&lt;/a&gt; - permutations of sets where no element appears in its original position. In the case of a six sibling gift exchange, there are 265 derangements. So now we have the denominator.&lt;/p&gt;
&lt;p&gt;The question is, of these derangements (I will never get tired of using that word, now that I know it), how many contain combinations where the siblings are mirrored, where each is assigned to the same person? To solve the puzzle, my friend Andrew and I took a three phase approach that we often use:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I try analytically and fail&lt;/li&gt;
&lt;li&gt;Andrew succeeds&lt;/li&gt;
&lt;li&gt;I confirm his answer with brute-force programming&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Andrew wrote, “I think the answer is 15/265. The denominator is a derangement, which are confusing as hell, but it is a type of permutation that is calculated: n! * Sum i = 0 -&amp;gt; n of ((-i)^n)/i!. The numerator I’m less sure about.I just counted the total ways that the matches would be mirrored. For person 1 and 6 to be matched there are 3 possibilities, and then there are 5 different people 1 could be matched with, so 5*3.&amp;quot;&lt;/p&gt;
&lt;p&gt;The combinations with person 1, aka Sara, and person 6, aka Lisa, look like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[(1,6)(2,5)(3,4)]&lt;/li&gt;
&lt;li&gt;[(1,6)(2,4)(3,5)]&lt;/li&gt;
&lt;li&gt;[(1,6)(2,3)(4,5)]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can then repeat that chain with siblings 2, 3, 4, and 5.&lt;/p&gt;
&lt;p&gt;To confirm, I started with all derangements and wrote a nested for-loop that looked for cases where the siblings are mirrored. The answer: 15/265, or approximately 5% of the time the gift exchange will work out the way it did this year. It’s another festivus miracle!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gtools)
library(tidyverse)

# First we get the derangements from the permutations
r &amp;lt;- 6
S &amp;lt;- 1:r
ders &amp;lt;- permutations(r, r, S)  
ders &amp;lt;- ders[apply(ders, 1, function(x) !any(x==S)),]

# This df is like a table 
# where the column name recieves from the row
ders &amp;lt;- as.data.frame(ders)
names(ders) &amp;lt;- c(1,2,3,4,5,6)

# start the counter
sibling_mirror &amp;lt;- 0

# iterate over rows of derangements
# and columns representing the recipient
for (giver in 1:nrow(ders)) {
  
  all_pairs &amp;lt;- list()
  
  for (recipient in names(ders)) {
    
    pair &amp;lt;- sort(c(ders[giver, recipient], recipient))
    
    all_pairs[[recipient]] &amp;lt;- pair
    
  }
  
  unique_pairs &amp;lt;- length(unique(all_pairs))
  
  # because I sort the pairs, 
  # if there are only 3 unique ones, that&amp;#39;s a mirror
  if (unique_pairs == 3) {
    sibling_mirror = sibling_mirror + 1
  }
  
}

sibling_mirror / nrow(ders)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Criminal Justice Reform: the Case of Utah</title>
      <link>/talk/dssg/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 -0700</pubDate>
      
      <guid>/talk/dssg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rayid Ghani on Data Science for Social Good</title>
      <link>/interview-rayid-ghani/</link>
      <pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/interview-rayid-ghani/</guid>
      <description>&lt;p&gt;I had a chance to sit down with Rayid Ghani, the Director of the Center for Data Science and Public Policy at the University of Chicago. I have admired Rayid’s work ever since he became the Chief Scientist for the 2012 Barack Obama campaign. His strategic use of analytics set the precedent for a new era of data science in political campaigns. At the time, others were trying to reverse engineer his tactics, which meant he was always one step ahead of the other campaigns. The fact that he has stayed in the field and used data to produce positive social outcomes is truly admirable.&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/XHDuN_2WP7o&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>How to Add a Logo to ggplot by Magick</title>
      <link>/ggplot-logo/</link>
      <pubDate>Tue, 11 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/ggplot-logo/</guid>
      <description>&lt;p&gt;We recently wanted to brand several of our plots for publication in the local press. I looked around and found a &lt;a href=&#34;https://stackoverflow.com/questions/12463691/inserting-an-image-to-ggplot-outside-the-chart-area&#34;&gt;couple&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/41574732/how-to-add-logo-on-ggplot2-footer&#34;&gt;suggestions&lt;/a&gt; on how to add images to plots, but nothing that seemed modular or customizable. My colleague reccomended the relatively new &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;Magick package&lt;/a&gt;, which provided all of the functionality I needed (plus a lot more). Here is a simple example along with the code to replicate it:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/Cars.png&#34; /&gt;

&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(magick)
library(here) # For making the script run without a wd
library(magrittr) # For piping the logo

# Make a simple plot and save it
ggplot(mpg, aes(displ, hwy, colour = class)) + 
  geom_point() + 
  ggtitle(&amp;quot;Cars&amp;quot;) +
  ggsave(filename = paste0(here(&amp;quot;/&amp;quot;), last_plot()$labels$title, &amp;quot;.png&amp;quot;),
         width = 5, height = 4, dpi = 300)

# Call back the plot
plot &amp;lt;- image_read(paste0(here(&amp;quot;/&amp;quot;), &amp;quot;Cars.png&amp;quot;))
# And bring in a logo
logo_raw &amp;lt;- image_read(&amp;quot;http://hexb.in/hexagons/ggplot2.png&amp;quot;) 

# Scale down the logo and give it a border and annotation
# This is the cool part because you can do a lot to the image/logo before adding it
logo &amp;lt;- logo_raw %&amp;gt;%
  image_scale(&amp;quot;100&amp;quot;) %&amp;gt;% 
  image_background(&amp;quot;grey&amp;quot;, flatten = TRUE) %&amp;gt;%
  image_border(&amp;quot;grey&amp;quot;, &amp;quot;600x10&amp;quot;) %&amp;gt;%
  image_annotate(&amp;quot;Powered By R&amp;quot;, color = &amp;quot;white&amp;quot;, size = 30, 
                 location = &amp;quot;+10+50&amp;quot;, gravity = &amp;quot;northeast&amp;quot;)

# Stack them on top of each other
final_plot &amp;lt;- image_append(image_scale(c(plot, logo), &amp;quot;500&amp;quot;), stack = TRUE)
# And overwrite the plot without a logo
image_write(final_plot, paste0(here(&amp;quot;/&amp;quot;), last_plot()$labels$title, &amp;quot;.png&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that this creates a logo centered on a grey background with an annotation originating in the northwest of the band. I wanted to put our logo on the bottom right, 538 style, so I added some superfulous canvas on the right of the png and then croped it to fit our plots with Magick. The point is, Magick is highly customizable and opens up a lot of options that were previously closed to all but the undaunted photoshoppers. Want Vincent Vega to point at your x-axis? Go for it! Now, if you’ll excuse me, I’m going to go home and have a heart attack.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/Cars_Travolta.gif&#34; /&gt;

&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now call back the plot
background &amp;lt;- image_read(paste0(here(&amp;quot;/&amp;quot;), &amp;quot;Cars.png&amp;quot;))
# And bring in a logo
logo_raw &amp;lt;- image_read(&amp;quot;https://i.imgur.com/e1IneGq.jpg&amp;quot;) 

frames &amp;lt;- lapply(logo_raw, function(frame) {
  image_composite(background, frame, offset = &amp;quot;+70+800&amp;quot;)
})

animation &amp;lt;- image_animate(image_join(frames))


image_write(animation, &amp;quot;~/Cars_Travolta.gif&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>RecidiViz: Analyzing Reincarceration</title>
      <link>/project/recidiviz/</link>
      <pubDate>Wed, 31 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/recidiviz/</guid>
      <description>&lt;p&gt;Recidivism is one of society&amp;rsquo;s most persistent yet misunderstood problems. Everyone from politicians to Supreme Court Justices seem to get it wrong. This model uses data from the Bureau of Justice Statistics to simulate 1,000 parolees released on the same day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Prison Analyst&#39;s Dilemma - Using R to Model Recidivism</title>
      <link>/recidiviz/</link>
      <pubDate>Wed, 31 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/recidiviz/</guid>
      <description>&lt;p&gt;Recidivism, the rate at which those released from incarceration return or commit new crimes, is one of society’s most difficult social problems. The official estimate is that 55% of former prisoners will return within 60 months.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://static1.squarespace.com/static/57e1fc9c20099e1414dc6070/t/592890393e00bedb90f8b8da/1495830599102/?format=1000w&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Recidivism is also, I discovered, one of the most challenging things to model and understand statistically. In this blog post, I describe our efforts to build &lt;a href=&#34;https://daniel-hadley.shinyapps.io/Recidivism_App/&#34;&gt;this simulation&lt;/a&gt;, including how we settled on some fairly basic control structures (for loops) without giving up too much in terms of efficiency and readability.&lt;/p&gt;
&lt;div id=&#34;a-common-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Common Problem&lt;/h3&gt;
&lt;p&gt;Recidivism is similar to many other types of social and ecological trends. There is a basic measurement - return to prison in our case, but it could also be website visits, weather patterns, bankruptcy, etc. But unlike, say, mortality rates, which can be modeled using &lt;a href=&#34;https://en.wikipedia.org/wiki/Survivorship_curve&#34;&gt;survivorship curves&lt;/a&gt;, a return to prison is not the end. The vast majority of people who recidivate will return to society, and then a portion of that group will return to prison again, and so on for many cycles.&lt;/p&gt;
&lt;p&gt;If this phenomenon sounds familiar, you may have thought of some type of model you have used in the past. And we looked at many of these, from &lt;a href=&#34;http://setosa.io/ev/markov-chains/&#34;&gt;Markov chains&lt;/a&gt; to state-based models. What we settled on is similar to an agent-based model, but is as simple as two for loops. In pseudocode, it looks like,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(agent in 1:number_of_agents) {
  for(month in 1:number_of_months) {
    Keep track of how long the person has been free
    Calculate the odds of recidivating conditional on months free
    If the agent recidivates, assign a prison sentence 
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this oversimplifies a lot. The “agents” don’t interact, and the probability of recidivating is conditional only on the time someone is free. But it does allow for multiple states, from freedom to prison and back again. And this - combined with the legibility of for loops - makes it easy to add complexity. In another variant of this model, for example, we also added parole sentences, adverse health outcomes, and many other variables.&lt;/p&gt;
&lt;p&gt;The basic functionality can be visualized in this tweenr plot created by my colleague, Sam Nelson:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/Sorenson-Impact/Recidivism_App/master/www/model_output.gif&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;for-hate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;For Hate&lt;/h3&gt;
&lt;p&gt;For loops are often pooh-poohed in the R community. They are thought of as slow and inferior to vectorized alternatives. Initially, I could see why. One of our more complex versions with 10,000 agents had build times of “let’s go out to lunch and see where it is when we get back.” When I wanted to do sensitivity analysis with multiple iterations, it was more like, “let’s sleep on it.” Even pared down, our initial version took 13 seconds.&lt;/p&gt;
&lt;p&gt;That’s why I was grateful for Grolemund and Wickham’s chapter on iteration in “R for Data Science.” First, they eased my [r]eligious guilt about our model structure: “Some people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years).”&lt;/p&gt;
&lt;p&gt;Then they provided some useful tips that allowed me to reduce the build time from 13 seconds to 3, without which we could not have built an interactive app. These were simple, but effective: preallocate the output, declare the type of the vector, and save the results in a list, waiting to combine them into a single vector until after the loop is done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-simple-to-complex&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;From Simple to Complex&lt;/h3&gt;
&lt;p&gt;I like this quote by the abolitionist Elizur Wright, “while nothing is more uncertain than a single life, nothing is more certain than the average duration of a thousand lives.” Recidivism trends are not as predictable, but I think that the same principle applies. That is why I like to build models that begin with very simple and uncertain functions, but that build to more tangible outcomes.&lt;/p&gt;
&lt;p&gt;I know that for loops do not come close to capturing the complexity inherent in social systems, but I like that that they mimic the hierarchy, from agents to averages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Sorenson-Impact/Recidivism_App&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Big Data to Solve Big Problems</title>
      <link>/big-data-big-problems/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/big-data-big-problems/</guid>
      <description>&lt;p&gt;In the marketing world, big data is used to answer ostensibly minute questions every day: are computer mouse movements predictive of purchasing? Does an orange background increase user engagement? In every &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_places_with_%22Silicon%22_names&#34;&gt;place with Silicon in its name&lt;/a&gt;, there are teams of data scientists asking these questions.&lt;/p&gt;
&lt;p&gt;In the social sector, by contrast, answering helpful questions is more difficult. For instance, is our program reducing homelessness? How is health spending distributed across the state? Part of the difficulty is the nature of the problems we are trying to solve. Lowering rates of recidivism is far more complex than driving ad clicks.&lt;/p&gt;
&lt;p&gt;But there is another reason that governments, nonprofits, and philanthropists find it difficult to answer basic questions about social services: missing data. We simply do not have the infrastructure to collect and analyze data that reveals true social outcomes.&lt;/p&gt;
&lt;p&gt;We at the Sorenson Impact Center are excited to help address this problem. Alongside &lt;a href=&#34;http://news.stanford.edu/2016/10/17/center-poverty-inequality-lead-program-measure-success-social-programs/&#34;&gt;Raj Chetty&lt;/a&gt; and some excellent organizations, we received a federal grant to help free up, merge, and analyze data to create better social policies. As part of the &lt;a href=&#34;https://nationalservice.tumblr.com/post/151761667357/how-data-and-innovation-can-help-people&#34;&gt;Pay for Success initiative&lt;/a&gt;, the White House has challenged us to assist service providers who will be paid &lt;em&gt;only if&lt;/em&gt; they produce good outcomes.&lt;/p&gt;
&lt;p&gt;The exciting part, from our perspective, is the possibility of using the same tools that were created to optimize things like ad performance in a totally different context. Silicon Valley’s emphasis on data science has led to a proliferation of free, open-source software, a great deal of which can be repurposed in support of Pay for Success projects. The trick is to know how to work with it.&lt;/p&gt;
&lt;p&gt;For instance, data scientists at Google, Twitter and Airbnb have released software that can help &lt;a href=&#34;https://google.github.io/CausalImpact/CausalImpact.html&#34;&gt;estimate causal impacts&lt;/a&gt;, &lt;a href=&#34;https://github.com/twitter/AnomalyDetection&#34;&gt;detect anomalies in time series data&lt;/a&gt;, and interactively &lt;a href=&#34;https://github.com/airbnb/caravel&#34;&gt;visualize large datasets&lt;/a&gt;, respectively. This is part of a broader movement to share code and create tools that others can use. Ultimately, this movement towards open source could give cash-strapped governments and nonprofits access to the same software resources as billion-dollar tech companies.&lt;/p&gt;
&lt;div id=&#34;the-bazaar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Bazaar&lt;/h3&gt;
&lt;p&gt;Open-source software has been described as a &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar&#34;&gt;metaphorical bazaar&lt;/a&gt;. As opposed to a Cathedral — where top-down decisions drive proprietary software — the Bazaar is a place where people from all over the globe can help fix bugs and generate a final product that is free thereafter. In our proposal to the federal government, we volunteered to serve as “guides to the Bazaar,” helping governments and nonprofits identify and learn how to utilize these rich resources.&lt;/p&gt;
&lt;p&gt;At the risk of sounding like techno-utopians, we think this approach of using big data tools has the potential to help solve some of our biggest social problems. One can imagine employing Artificial Neural Networks to identify the parolees who are most in need of social work, or the students who need advanced tutoring. At the very least, we can use packages in the programming languages Python and R to transfer and merge data so that it is useful to service providers.&lt;/p&gt;
&lt;p&gt;If this work sounds appealing to you, please reach out. We are hiring data scientists.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Dashboard with Weak AI</title>
      <link>/project/dashboards/</link>
      <pubDate>Sat, 15 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/dashboards/</guid>
      <description>&lt;p&gt;R has been the perfect language for the back end of &lt;a href=&#34;http://www.somervillema.gov/dashboard/daily.html&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; government data dashboard I am developing.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It has excellent packages to pipe in data from every significant source&lt;/li&gt;
&lt;li&gt;Tools like dplyr and tidyr make cleaning and munging data trivial&lt;/li&gt;
&lt;li&gt;It is ideal for automating analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the R script that powers my dashboard, I have everything from simple averages and frequency tables, to a complex algorithm that converts timeseries figures to Z-Scores and then selects the top 3 variables to display based on standard scores from the last 7 days. (Today that happens to include an uptick in reports of dead animals on the streets and sidewalks, which is disgusting but useful to know).&lt;/p&gt;

&lt;p&gt;This blogpost describes the technical details, and the dashboard&amp;rsquo;s repository is &lt;a href=&#34;https://github.com/cityofsomerville/SomervilleSystems&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Jupyter Notebooks to R Markdown</title>
      <link>/jupyter-to-rmarkdown/</link>
      <pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/jupyter-to-rmarkdown/</guid>
      <description>&lt;p&gt;Edit, 3/28/18: RStudio just announced Python interoperability through the &lt;a href=&#34;https://blog.rstudio.com/2018/03/26/reticulate-r-interface-to-python/&#34;&gt;reticulate package&lt;/a&gt;. Rmd Notebooks are unbeatable, in my opinion.&lt;/p&gt;
&lt;div id=&#34;original-post&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Original Post:&lt;/h3&gt;
&lt;p&gt;I started using Jupyter Notebooks back when they were called IPython. I even remember having to set up a virtual Linux environment because they were not available on Windows. As much as I have enjoyed their functionality, I recently switched entirely to R Markdown in an RStudio environment. Here’s why.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-to-github&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exporting to Github&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;http://rmarkdown.rstudio.com/github_document_format.html&#34;&gt;github_document&lt;/a&gt; option in R Markdown is the perfect way to share your analysis over the web. Just write the markdown with R chunks, “knit” it, and commit everything to a repository, and RStudio will create a directory with all plots and outputs ready to view. One advantage Jupyter Notebooks used to hold was the aesthetic formatting of tables and dataframes on Github. But with the kable function in the knitr package, I have been able to output HTML tables that look just as clean.&lt;/p&gt;
&lt;p&gt;I updated a previous Gist to show how this works &lt;a href=&#34;https://github.com/DanielHadley/Example-R-Markdown-on-Github/blob/master/Example.md&#34;&gt;on Github&lt;/a&gt;. Compare that finished product with the raw code &lt;a href=&#34;https://raw.githubusercontent.com/DanielHadley/Example-R-Markdown-on-Github/master/Example.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-to-word-is-a-breeze&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exporting to Word is a breeze&lt;/h3&gt;
&lt;p&gt;A lot of my analysis is for non-technical clients, many of whom are most comfortable with a Word doc. I have found the process of translating code to .docx to be exquisitely easy. The hardest part was inserting page breaks, but thankfully someone come up with a &lt;a href=&#34;https://scriptsandstatistics.wordpress.com/2015/12/18/rmarkdown-how-to-inserts-page-breaks-in-a-ms-word-document/&#34;&gt;clever hack&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;version-control&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Version control&lt;/h3&gt;
&lt;p&gt;As &lt;a href=&#34;http://opiateforthemass.es/articles/why-i-dont-like-jupyter-fka-ipython-notebook/&#34;&gt;others&lt;/a&gt; have pointed out, version control of a Jupyther Notebook is not a straightforward process. R Markdown is all plain text, so changes are easy to track in Git. For the first time, I feel in control of changes made to my Word documents.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;RStudio&lt;/h3&gt;
&lt;p&gt;I had the R kernel working nicely in Jupyter Notebooks. I did not encounter a lot of errors. But the entire time I was developing, I missed all of the features of RStudio. In particular, I missed having tab completion and the correct indentation on dplyr chains. But I also missed the layout with my environment, history, console, etc., not to mention the View() function on dataframes, and running code line-by-line. RStudio has become an amazing, feature-rich IDE, which I am not anxious to trade for another environment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-is-here-sort-of&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Python is here (sort of)&lt;/h3&gt;
&lt;p&gt;At least since &lt;a href=&#34;https://ironholds.org/blog/python-now-in-rstudio/&#34;&gt;2004&lt;/a&gt;, it is possible to run Python scripts in RStudio. There is even syntax highlighting and other IDE features. The nice thing is, the knitr library has support for &lt;a href=&#34;http://yihui.name/knitr/demo/engines/&#34;&gt;other languages&lt;/a&gt;, so you can include Python chunks in, for example, your Word doc.&lt;/p&gt;
&lt;p&gt;There are problems with this, as &lt;a href=&#34;https://www.reddit.com/r/rstats/comments/51tzcb/why_i_switched_from_jupyter_notebooks_to_r/d7k3zvd&#34;&gt;u/_Wintermute noted&lt;/a&gt;. Apparently, the variables do not transfer between “chunks,” let alone languages. The docs for the Python &lt;a href=&#34;http://rmarkdown.rstudio.com/authoring_knitr_engines.html&#34;&gt;language engine&lt;/a&gt; describe a data exchange, which is exciting, but it looks like you have to write the dataframe in one language and then read it in the other. These are not exactly seamless transitions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unknown-unknowns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unknown Unknowns&lt;/h3&gt;
&lt;p&gt;I’m certain there are a lot of great features of both platforms that I am totally unaware of. And I still think Jupyter is doing more to integrate the various data science languages. It’s possible I will switch back, but for now I seem to be getting more locked in to RStudio. At the moment I am researching how to convert my website into R Markdown. It just works really well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lessons From My First Kaggle Contest</title>
      <link>/kaggle-thoughts/</link>
      <pubDate>Thu, 09 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/kaggle-thoughts/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt; is a forum for interacting with other data scientists and competing to see who can write code that will best predict features of data. It’s a way to test your skills at statistics and machine learning, and to do a lot of human learning in the process (sorry, bad pun).&lt;/p&gt;
&lt;p&gt;When I entered the contest to categorize crimes that occurred in San Francisco, my initial goal was to do better than random chance. I had some domain expertise from my time working with the Somerville Police Department, but I still worried that I would not be able to beat basic benchmarks for crime prediction. Over time, my goals shifted from making it out of the bottom quartile, to scoring above the median, until I finally made it into the top 10% and set my sights on winning. In the end, I ranked 7th out of 2,335 competitors.&lt;/p&gt;
&lt;p&gt;There was no prize money in this contest, and I borrowed liberally from others’ ideas, so more than the ranking, I got involved to learn new things about data science.&lt;/p&gt;
&lt;div id=&#34;feature-engineering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Feature Engineering&lt;/h3&gt;
&lt;p&gt;Put simply, this is the process of taking rows and columns of data, and making new ones that make the predictive models more powerful. For example, one of the most important features in the contest to categorize crime is when it was reported. Did it take place on a week, or weekend? Was it reported at the same time as another crime? Did it occur during the day, or at night? Answers to these questions were hiding in rows and columns of data, and my most productive time was spent discovering them.&lt;/p&gt;
&lt;p&gt;It’s cliche, but feature engineering ended up mattering much more than model selection or parameter tuning. This is also where I had a slight edge, having sat through dozens of CompStat meetings with the Chief, his command staff, and a really good crime analyst.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models&lt;/h3&gt;
&lt;p&gt;In the end, I broke my task up into two R scripts:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;engineer_features.R&lt;/li&gt;
&lt;li&gt;build_models.R&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;XGBoost (which a work friend pointed out sounds like a bad energy drink) ended up being every bit as powerful and simple as many data scientists describe it. From the time I started to use it instead of other popular algorithms, my errors started to decrease significantly.&lt;/p&gt;
&lt;p&gt;I created a method of iterating through a list of each crime and building a separate model to predict only that crime. This would often work better than the models that I trained on the entire set of categories. Here is some pseudocode to demonstrate the concept:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The list to iterate over
crime_categories &amp;lt;- c(levels(train$Category))

for (crime in 1:length(crime_categories)) {
  
  category_to_model &amp;lt;- crime_categories[crime]
  
  # Get it ready for the model
  train_final %&amp;gt;% 
    mutate(category_binary = ifelse(Category == category_to_model, 1, 0))
    
  
  xgboost_model &amp;lt;- xgboost(param = param, data = train_final[, -c(26)], label = train_final[, c(26)], 
                           nrounds = nrounds_vector[crime])
  
  # Predict
  pred &amp;lt;- predict(xgboost_model, test_final)
  
  prob &amp;lt;- matrix(pred, ncol = 1, byrow = T)
  prob &amp;lt;- as.data.frame(prob)
  colnames(prob)  &amp;lt;- category_to_model
  prob$Id &amp;lt;- as.numeric(seq(1 : 884262) -1)
  
  final_predictions[,crime + 1] &amp;lt;- prob[,1]
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the final hours of the contest, I also combined the results of several different submissions. Together, they did better than any single submission.&lt;/p&gt;
&lt;p&gt;I made a lot of mistakes along the way, from overfitting dozens of models to accidentally purchasing a surfeit of storage on Amazon Web Services. These were instructive, though, and I feel that the process gave me a more nuanced view of how computers learn from the code and data that we feed them.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
